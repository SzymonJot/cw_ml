---
title: "projekt_ML_4"
author: "Szymon Jarmołowski"
format: 
  html:
    self-contained: true
    embedded-resources: true
    toc: true
    toc-depth: 4
    toc-location: right
    toc-title: "Spis treści"
    number-sections: true
    number-depth: 4
    code-fold: show
    code-summary: "Show the code"
    code-tools: true
    code-block-bg: true
    code-block-border-left: "black"
    code-line-numbers: false
    code-copy: true
    html-math-method: katex
    smooth-scroll: true
    anchor-sections: true
    link-external-icon: true
    link-external-newwindow: true
---

# Optymalizacja hipterparametrów

## Import bibliotek
```{r}
#| warning: false
library(tidymodels)

# Dodatkowe pakiety
library(rpart.plot)  # wizualizacja drzew decyzyjnych 
library(vip)         # wykres wagi zmiennych

```

```{r}
data("cells", package = "modeldata")
cells
```

```{r}
set.seed(123)
split <- initial_split(data = cells |> select(-case), 
                       prop = 3/4, 
                       strata = class)

train <- training(split)
test <- testing(split)
```

```{r}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(), 
    tree_depth = tune()) |> 
  set_engine("rpart") |> 
  set_mode("classification")

tune_spec
```


```{r}
siatka <- grid_regular(cost_complexity(), 
                       tree_depth(), 
                       levels = 5)
siatka
```

```{r}
# podgląd parametrów 

siatka |> 
  count(tree_depth)
```

```{r}
siatka |> 
  count(cost_complexity)
```

```{r}
set.seed(234)
folds <- vfold_cv(train)
```

```{r}
set.seed(345)

# workflow

work <- 
  workflow() |> 
  add_model(tune_spec) |> 
  add_formula(class ~ .)

# statystyki oceny dokładnosci modelu 

miary_oceny <-
  yardstick::metric_set(# tym parametrem możesz definiować
    accuracy,
    mcc,
    npv,
    roc_auc)

# Optymalizacja 

fit_tree <-
  work |>
  tune_grid(
    resamples = folds,
    grid = siatka,
    metrics = miary_oceny
  )

fit_tree
```

```{r}
fit_tree |> collect_metrics()
```

```{r}
fit_tree %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

```{r}
fit_tree |> show_best(metric = "accuracy")
```

```{r}
fit_tree |> select_best(metric = "accuracy")
```

```{r}
best_mod <- fit_tree |> select_best(metric = "accuracy")

final_mod <-  
  work |> 
  finalize_workflow(best_mod)
```

```{r}
final_fit <- 
  final_mod |> 
  last_fit(split = split)

final_fit %>%
  collect_metrics()
```

```{r}
final_fit |> 
  collect_predictions() |> 
  roc_curve(truth = class, .pred_PS) |> 
  autoplot()
```


```{r}
final_fit |> extract_workflow()
```

```{r}
final_fit |> 
  extract_workflow() |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = F)
```

```{r}
# wykres 

final_fit |> 
  extract_workflow() |> 
  extract_fit_parsnip() |>
  vip() 
```

```{r}
# eksport danych do tabeli

final_fit |>
  extract_workflow() |>
  extract_fit_parsnip() |>
  vip() |> 
  _$data |> 
  knitr::kable(digits = 1)
```
```{r}
args(decision_tree)
```

```{r}
?decision_tree()
```


## Rozszerzona specyfikacja modelu
```{r}
tune_spec_extended <- 
  decision_tree(
    cost_complexity = tune(), 
    tree_depth = tune(),
    min_n = tune() 
  ) |> 
  set_engine("rpart") |> 
  set_mode("classification")

tune_spec_extended
```


```{r}
grid <- grid_regular(
  cost_complexity(range = c(-4, -1)), 
  tree_depth(range = c(1, 15)), 
  min_n(range = c(2, 40)),
  levels = c(4,4,3)
)

grid
```

```{r}
# Parametry
grid |> 
  count(tree_depth, min_n) |> 
  arrange(tree_depth, min_n)
```

## Workflow z rozszerzonym modelem
```{r}
set.seed(456)
work_extended <- 
  workflow() |> 
  add_model(tune_spec_extended) |> 
  add_formula(class ~ .)

# Dostrajanie z rozszerzoną siatką
fit_tree_extended <-
  work_extended |>
  tune_grid(
    resamples = folds,
    grid = grid,
    metrics = miary_oceny
  )

fit_tree_extended
```

## Analiza wyników rozszerzonego dostrajania
```{r}
fit_tree_extended |> collect_metrics()
```

```{r}
# Najlepsze modele 
fit_tree_extended |> show_best(metric = "accuracy", n = 5)
```

```{r}
fit_tree_extended |> show_best(metric = "roc_auc", n = 5)
```


## Wizualizacja wyników 
```{r}
fit_tree_extended %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  ggplot(aes(cost_complexity, mean, color = factor(tree_depth))) +
  geom_line(alpha = 0.7) +
  geom_point() +
  facet_wrap(~ min_n, labeller = label_both) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(name = "Głębokość drzewa") +
  labs(title = "Dokładność vs Koszt Złożoności",
       x = "Koszt złożoności",
       y = "Średnia Dokładność")
```

```{r}
# Wizualizacja wpływu min_n
fit_tree_extended %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  ggplot(aes(min_n, mean, color = factor(tree_depth))) +
  geom_line(alpha = 0.7) +
  geom_point() +
  facet_wrap(~ cost_complexity, scales = "free_x", labeller = label_both) +
  scale_color_viridis_d(name = "Głębokość drzewa") +
  labs(title = "Dokładność vs min_n",
    x = "Minimalna liczba obserwacji w liściu",
y = "Średnia dokładność")
```

## Porównanie podstawowej i rozszerzonej wersji
```{r}
# Najlepszy model z podstawowego tuningu
best_basic <- fit_tree |> select_best(metric = "accuracy")
best_basic_perf <- fit_tree |> 
  collect_metrics() |> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 1)

# Najlepszy model z rozszerzonego tuningu  
best_extended <- fit_tree_extended |> select_best(metric = "accuracy")
best_extended_perf <- fit_tree_extended |> 
  collect_metrics() |> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, n = 1)

# Porównanie
comparison <- bind_rows(
  best_basic_perf |> mutate(tuning = "Podstawowe (2 parametry)"),
  best_extended_perf |> mutate(tuning = "Rozszerzone (3 parametry)")
) |> 
  select(tuning, mean, std_err, cost_complexity, tree_depth, min_n)

comparison
```
Wyniki z kros walidacją pokazują lepsze średnie 
## Najlepszy model
```{r}
final_mod_extended <-  
  work_extended |> 
  finalize_workflow(best_extended)

final_fit_extended <- 
  final_mod_extended |> 
  last_fit(split = split)


final_fit_extended %>%
  collect_metrics()
```

## Porównanie końcowych modeli
```{r}
# Metryki z podstawowego modelu
basic_metrics <- final_fit %>% collect_metrics() %>% mutate(model = "Podstawowa")

# Metryki z rozszerzonego modelu  
extended_metrics <- final_fit_extended %>% collect_metrics() %>% mutate(model = "Rozszerzona")

# Porównanie
final_comparison <- bind_rows(basic_metrics, extended_metrics) %>%
  select(model, .metric, .estimate) %>%
  pivot_wider(names_from = model, values_from = .estimate) %>%
  mutate(Wzrost = Rozszerzona - Podstawowa)

final_comparison
```

Model końcowy okazał się jednak gorszy po dostosowaniu parametrów.

## Wizualizacja drzewa
```{r}
final_fit_extended |> 
  extract_workflow() |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE, 
             main = "Rozszerzona wersja")
```